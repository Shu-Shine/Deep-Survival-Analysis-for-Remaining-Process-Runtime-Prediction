{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# DeepSurv for Remaining Process Runtime Prediction\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# optional install\n",
    "\n",
    "# import pip\n",
    "# def install(package):\n",
    "#     pip.main([\"install\",package])\n",
    "# install(\"pm4py\")\n",
    "\n",
    "# install(\"missingno\")\n",
    "# import missingno as msno\n",
    "\n",
    "# install(\"pycox\")\n",
    "# install(\"sklearn\")\n",
    "# install(\"torch\")\n",
    "# install(\"sklearn_pandas \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "\n",
    "from pycox.datasets import metabric\n",
    "from pycox.models import CoxPH\n",
    "from pycox.evaluation import EvalSurv\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "_ = torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data preprocess for case attributes\n",
    "\n",
    "Event attributes data is processed using Prefix length bucketing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Action</th>\n",
       "      <th>org:resource</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>EventOrigin</th>\n",
       "      <th>EventID</th>\n",
       "      <th>lifecycle:transition</th>\n",
       "      <th>time:timestamp</th>\n",
       "      <th>case:LoanGoal</th>\n",
       "      <th>case:ApplicationType</th>\n",
       "      <th>case:concept:name</th>\n",
       "      <th>case:RequestedAmount</th>\n",
       "      <th>FirstWithdrawalAmount</th>\n",
       "      <th>NumberOfTerms</th>\n",
       "      <th>Accepted</th>\n",
       "      <th>MonthlyCost</th>\n",
       "      <th>Selected</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>OfferedAmount</th>\n",
       "      <th>OfferID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Created</td>\n",
       "      <td>User_1</td>\n",
       "      <td>A_Create Application</td>\n",
       "      <td>Application</td>\n",
       "      <td>Application_652823628</td>\n",
       "      <td>complete</td>\n",
       "      <td>2016-01-01 09:51:15.304000+00:00</td>\n",
       "      <td>Existing loan takeover</td>\n",
       "      <td>New credit</td>\n",
       "      <td>Application_652823628</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>statechange</td>\n",
       "      <td>User_1</td>\n",
       "      <td>A_Submitted</td>\n",
       "      <td>Application</td>\n",
       "      <td>ApplState_1582051990</td>\n",
       "      <td>complete</td>\n",
       "      <td>2016-01-01 09:51:15.352000+00:00</td>\n",
       "      <td>Existing loan takeover</td>\n",
       "      <td>New credit</td>\n",
       "      <td>Application_652823628</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Created</td>\n",
       "      <td>User_1</td>\n",
       "      <td>W_Handle leads</td>\n",
       "      <td>Workflow</td>\n",
       "      <td>Workitem_1298499574</td>\n",
       "      <td>schedule</td>\n",
       "      <td>2016-01-01 09:51:15.774000+00:00</td>\n",
       "      <td>Existing loan takeover</td>\n",
       "      <td>New credit</td>\n",
       "      <td>Application_652823628</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deleted</td>\n",
       "      <td>User_1</td>\n",
       "      <td>W_Handle leads</td>\n",
       "      <td>Workflow</td>\n",
       "      <td>Workitem_1673366067</td>\n",
       "      <td>withdraw</td>\n",
       "      <td>2016-01-01 09:52:36.392000+00:00</td>\n",
       "      <td>Existing loan takeover</td>\n",
       "      <td>New credit</td>\n",
       "      <td>Application_652823628</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Created</td>\n",
       "      <td>User_1</td>\n",
       "      <td>W_Complete application</td>\n",
       "      <td>Workflow</td>\n",
       "      <td>Workitem_1493664571</td>\n",
       "      <td>schedule</td>\n",
       "      <td>2016-01-01 09:52:36.403000+00:00</td>\n",
       "      <td>Existing loan takeover</td>\n",
       "      <td>New credit</td>\n",
       "      <td>Application_652823628</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Action org:resource            concept:name  EventOrigin  \\\n",
       "0      Created       User_1    A_Create Application  Application   \n",
       "1  statechange       User_1             A_Submitted  Application   \n",
       "2      Created       User_1          W_Handle leads     Workflow   \n",
       "3      Deleted       User_1          W_Handle leads     Workflow   \n",
       "4      Created       User_1  W_Complete application     Workflow   \n",
       "\n",
       "                 EventID lifecycle:transition  \\\n",
       "0  Application_652823628             complete   \n",
       "1   ApplState_1582051990             complete   \n",
       "2    Workitem_1298499574             schedule   \n",
       "3    Workitem_1673366067             withdraw   \n",
       "4    Workitem_1493664571             schedule   \n",
       "\n",
       "                     time:timestamp           case:LoanGoal  \\\n",
       "0  2016-01-01 09:51:15.304000+00:00  Existing loan takeover   \n",
       "1  2016-01-01 09:51:15.352000+00:00  Existing loan takeover   \n",
       "2  2016-01-01 09:51:15.774000+00:00  Existing loan takeover   \n",
       "3  2016-01-01 09:52:36.392000+00:00  Existing loan takeover   \n",
       "4  2016-01-01 09:52:36.403000+00:00  Existing loan takeover   \n",
       "\n",
       "  case:ApplicationType      case:concept:name  case:RequestedAmount  \\\n",
       "0           New credit  Application_652823628               20000.0   \n",
       "1           New credit  Application_652823628               20000.0   \n",
       "2           New credit  Application_652823628               20000.0   \n",
       "3           New credit  Application_652823628               20000.0   \n",
       "4           New credit  Application_652823628               20000.0   \n",
       "\n",
       "   FirstWithdrawalAmount  NumberOfTerms Accepted  MonthlyCost Selected  \\\n",
       "0                    NaN            NaN      NaN          NaN      NaN   \n",
       "1                    NaN            NaN      NaN          NaN      NaN   \n",
       "2                    NaN            NaN      NaN          NaN      NaN   \n",
       "3                    NaN            NaN      NaN          NaN      NaN   \n",
       "4                    NaN            NaN      NaN          NaN      NaN   \n",
       "\n",
       "   CreditScore  OfferedAmount OfferID  \n",
       "0          NaN            NaN     NaN  \n",
       "1          NaN            NaN     NaN  \n",
       "2          NaN            NaN     NaN  \n",
       "3          NaN            NaN     NaN  \n",
       "4          NaN            NaN     NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(df):\n",
    "    data = df.copy()\n",
    "\n",
    "    # convert timestamp to datetime format\n",
    "    data['time:timestamp'] = pd.to_datetime(data['time:timestamp'])\n",
    "    data.sort_values(['case:concept:name', 'time:timestamp'], inplace=True)\n",
    "\n",
    "    # calculate the whole process duration\n",
    "    data['duration'] = data.groupby('case:concept:name')['time:timestamp'].transform(lambda x: x.max() - x.min())\n",
    "    \n",
    "    # create feature vector\n",
    "    feature = data.drop_duplicates(subset=['case:concept:name'])\n",
    "\n",
    "    # process case attributes\n",
    "    feature.drop(columns=['Action','org:resource','concept:name','EventID','EventOrigin','lifecycle:transition','time:timestamp','OfferID', \n",
    "                          'FirstWithdrawalAmount','NumberOfTerms','Accepted','MonthlyCost','Selected','CreditScore','OfferedAmount'], inplace=True)\n",
    "    \n",
    "    feature['duration'] = feature['duration'].dt.total_seconds() / 86400.0\n",
    "    \n",
    "    # introduce event indicator\n",
    "    event_indicator = data[data['concept:name'] == 'A_Pending']['case:concept:name'].unique()\n",
    "    feature['event'] = feature['case:concept:name'].isin(event_indicator).astype(int)\n",
    "    \n",
    "    # one-hot coding \n",
    "    feature = pd.get_dummies(feature, columns=['case:LoanGoal', 'case:ApplicationType'])\n",
    "    \n",
    "     # save the feature from case attributes\n",
    "    feature.to_csv('feature.csv', index=False)\n",
    "    \n",
    "    return feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bucket_and_merge(t, feature):\n",
    "    # load buckets to get event attributes \n",
    "    bucket = pd.read_csv(f'b/data{t}.csv')\n",
    "\n",
    "    # merge with feature from case attributes\n",
    "    bucket = pd.merge(bucket, feature, on='case:concept:name', how='left')\n",
    "\n",
    "    return bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(bucket):\n",
    "    df_test = bucket.sample(frac=0.2, random_state=10)\n",
    "    df_comp = df_test[df_test['event'] == 1]\n",
    "    df_train = bucket.drop(df_test.index)\n",
    "    df_val = df_train.sample(frac=0.2, random_state=11)\n",
    "    df_train = df_train.drop(df_val.index)\n",
    "\n",
    "    return df_train, df_val, df_test, df_comp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_transforms(df_train, df_val, df_test, df_comp):\n",
    "    # standardize the numerical covariates, leave the binary variables as is\n",
    "    cols_standardize = ['Duration','last_dur','Created','Deleted','Obtained','Released','statechange','Application',\n",
    "                        'Offer','Workflow',\n",
    "                       'case:RequestedAmount', 'FirstWithdrawalAmount','CreditScore', 'OfferedAmount',\n",
    "                        'A_Complete','A_Concept','A_Create Application','A_Incomplete',\n",
    "                        'A_Submitted','A_Validating','O_Accepted','O_Cancelled','O_Create Offer','O_Created',\n",
    "                        'O_Returned','O_Sent (mail and online)','O_Sent (online only)',\n",
    "                        'W_Call after offers','W_Call incomplete files',\n",
    "                        'W_Complete application','W_Handle leads','W_Validate application']\n",
    "\n",
    "    cols_leave = ['case:LoanGoal_Boat', 'case:LoanGoal_Business goal',\n",
    "        'case:LoanGoal_Car', 'case:LoanGoal_Caravan / Camper',\n",
    "        'case:LoanGoal_Debt restructuring',\n",
    "        'case:LoanGoal_Existing loan takeover',\n",
    "        'case:LoanGoal_Extra spending limit', 'case:LoanGoal_Home improvement',\n",
    "        'case:LoanGoal_Motorcycle', 'case:LoanGoal_Not speficied',\n",
    "        'case:LoanGoal_Other, see explanation','NumberOfTerms',\n",
    "        'case:LoanGoal_Remaining debt home', 'case:LoanGoal_Tax payments',\n",
    "        'case:LoanGoal_Unknown',  'Accepted','case:ApplicationType_Limit raise','case:ApplicationType_New credit',\n",
    "            'Selected'] \n",
    "\n",
    "    standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "    leave = [(col, None) for col in cols_leave]\n",
    "    x_mapper = DataFrameMapper(standardize + leave)\n",
    "\n",
    "    # variables needs to be of type 'float32', as required by pytorch\n",
    "    x_train = x_mapper.fit_transform(df_train).astype('float32')\n",
    "    x_val = x_mapper.transform(df_val).astype('float32')\n",
    "    x_test = x_mapper.transform(df_test).astype('float32')\n",
    "    x_comp = x_mapper.transform(df_comp).astype('float32')\n",
    "    \n",
    "    return x_train, x_val, x_test, x_comp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning-based survival analysis model\n",
    "\n",
    "DeepSurv is primarily extends the Cox Proportional Hazards model, by incorporating a deep neural network to learn intricate patterns and relationships from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, y_train, val):\n",
    "    # Deep neural net\n",
    "    in_features = x_train.shape[1]\n",
    "    num_nodes = [128, 128, 128, 128]\n",
    "    out_features = 1\n",
    "    batch_norm = True\n",
    "    dropout = 0.4 \n",
    "    output_bias = False\n",
    "\n",
    "    x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "    val = tuple(data.to(device) for data in val)\n",
    "\n",
    "    net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm, dropout, output_bias=output_bias)\n",
    "    \n",
    "    # Training the model, using the `Adam` optimizer\n",
    "    model = CoxPH(net, tt.optim.Adam).to(device)\n",
    "    batch_size = 128\n",
    "\n",
    "    # Instead of choosing a learning rate, use the scheme proposed by [Smith 2017](https://arxiv.org/pdf/1506.01186.pdf) \n",
    "    # to find a suitable learning rate with `model.lr_finder`. \n",
    "    # See [this post](https://towardsdatascience.com/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6) for an explanation.\n",
    "    lrfinder = model.lr_finder(x_train, y_train, batch_size, tolerance=10)\n",
    "    best_lr = lrfinder.get_best_lr()\n",
    "\n",
    "    # We include the `EarlyStopping` callback to stop training when the validation loss stops improving. \n",
    "    # After training, this callback will also load the best performing model in terms of validation loss.\n",
    "    callbacks = [tt.callbacks.EarlyStopping()]\n",
    "    verbose = True\n",
    "    log = model.fit(x_train, y_train, batch_size, 512, callbacks, verbose, val_data=val, val_batch_size=batch_size)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Multiple predictor approach\n",
    "\n",
    "Build one predictor for each bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mae_list = []\n",
    "random_state = 42\n",
    "\n",
    "feature = preprocess_data(df)\n",
    "\n",
    "for t in range(1, 21):\n",
    "\n",
    "    # load buckets\n",
    "    bucket = load_bucket_and_merge(t, feature)\n",
    "\n",
    "    # data split\n",
    "    df_train, df_val, df_test, df_comp = split_data(bucket)\n",
    "\n",
    "    # Feature transforms\n",
    "    x_train, x_val, x_test, x_comp = feature_transforms(df_train, df_val, df_test, df_comp)\n",
    "\n",
    "    get_target = lambda df: (df['duration'].values, df['event'].values)\n",
    "    y_train = get_target(df_train)\n",
    "    y_val = get_target(df_val)\n",
    "    durations_train, events_train = get_target(df_train)\n",
    "    durations_val, events_val = get_target(df_val)\n",
    "    durations_test, events_test = get_target(df_test)\n",
    "    durations_comp, events_comp = get_target(df_comp)\n",
    "    val = x_val, y_val\n",
    "\n",
    "    # Training the model\n",
    "    model = train_model(x_train, y_train, val)\n",
    "\n",
    "    # get the partial log-likelihood\n",
    "    model.partial_log_likelihood(*val).mean()\n",
    "\n",
    "    # get the non-parametric baseline hazard estimates\n",
    "    _ = model.compute_baseline_hazards()\n",
    "\n",
    "    # Prediction of DeepSurv, returning the survival estimates as a dataframe\n",
    "    s_val = model.predict_surv_df(x_val)\n",
    "    s_train = model.predict_surv_df(x_train)\n",
    "\n",
    "    # determine the threshod by grid search\n",
    "    thresholds = np.linspace(0.4, 1, num=100)\n",
    "    best_val = None\n",
    "    min_loss_v = float('inf')\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        survival_v = s_val.apply(lambda col: (col <= threshold).idxmax())\n",
    "        mae_v = np.mean(np.abs(survival_v - durations_val))\n",
    "\n",
    "        if mae_v < min_loss_v:\n",
    "            min_loss_v = mae_v\n",
    "            best_val = threshold\n",
    "\n",
    "    print(\"Best Threshold:\", best_val)\n",
    "\n",
    "    # Prediction on testset\n",
    "    surv = model.predict_surv_df(x_comp)\n",
    "    threshold = best_val\n",
    "    survival_time = []\n",
    "    survival_time = surv.apply(lambda col: (col <= threshold).idxmax())\n",
    "\n",
    "    mae = np.mean(np.abs(survival_time - durations_comp))\n",
    "    print(\"mae\", mae)\n",
    "    mae_list.append(mae)\n",
    "\n",
    "print('averge MAE', np.mean(mae_list))\n",
    "print('std', np.std(mae_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
